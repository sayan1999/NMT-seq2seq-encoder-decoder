{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bidict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, os\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "import tensorflow as tf\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from bidict import bidict\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "training=False\n",
    "ctx_vec_len=256\n",
    "epochs=1\n",
    "no_of_tests=10\n",
    "# if running on colab turn this false, and select GPU runtime\n",
    "colab=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Architecture For Neural Machine Trans\n",
       "![Architecture Neural Machine Trans](image/NeuralMachineTrans.png)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "if not colab:\n",
    "    display(Markdown('''## Architecture For Neural Machine Trans\n",
    "![Architecture Neural Machine Trans](image/NeuralMachineTrans.png)'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not colab:\n",
    "    # if on local machine \n",
    "    root_dir='.'\n",
    "    \n",
    "else:\n",
    "    # if using google colab use this code\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    root_dir = \"/content/drive/My Drive/Colab Notebooks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(root_dir, \"fra.csv\")\n",
    "doc = pd.read_csv(data_path, skiprows = range(1, 130000), nrows=10000)\n",
    "# shuffle dataset\n",
    "doc=doc.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace contracted forms for english words\n",
    "contracted_dict={\"won't\" : \"will not\", \"can\\'t\" : \"can not\", \"n\\'t\" : \" not\", \"\\'re\" : \" are\", \"\\'s\" : \" is\", \"\\'d\" : \" would\", \"\\'ll\" : \" will\", \"\\'t\" : \" not\", \"\\'ve\" : \" have\", \"\\'m\" : \" am\"}\n",
    "\n",
    "def replace_contracted(text):\n",
    "\n",
    "    regex = re.compile(\"|\".join(map(re.escape, contracted_dict.keys(  ))))\n",
    "    return regex.sub(lambda match: contracted_dict[match.group(0)], text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply decontraction and lowercase\n",
    "doc=doc.apply(np.vectorize(lambda sent : replace_contracted(str(sent).strip().lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize sentences and add start_ and _end keyword to target sentences\n",
    "source_sents=doc.Source.apply(lambda sent: word_tokenize(sent))\n",
    "target_sents=doc.Target.apply(lambda x : 'START_ '+ x + ' _END').apply(lambda sent: word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# building the vocabulary\n",
    "source_vocab=set().union(*source_sents)\n",
    "target_vocab=set().union(*target_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max sentence length for each language in the dataset\n",
    "max_source_len=max(source_sents.apply(len))\n",
    "max_target_len=max(target_sents.apply(len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric identity for each word in vocab\n",
    "source_wordint_rel=bidict(enumerate(source_vocab, 1))\n",
    "target_wordint_rel=bidict(enumerate(target_vocab, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare inputs and outputs\n",
    "encoder_source_arr=[list(map(lambda word : source_wordint_rel.inv[word], sent)) for sent in source_sents]\n",
    "decoder_source_arr=[list(map(lambda word : target_wordint_rel.inv[word], sent)) for sent in target_sents]\n",
    "decoder_output_arr=[list(map(lambda word : target_wordint_rel.inv[word], sent[1:])) for sent in target_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pad the inputs and outputs to max length\n",
    "padded_encoder_source_arr=pad_sequences(encoder_source_arr, maxlen=max_source_len, padding='post')\n",
    "padded_decoder_source_arr=pad_sequences(decoder_source_arr, maxlen=max_target_len, padding='post')\n",
    "padded_decoder_output_arr=pad_sequences(decoder_output_arr, maxlen=max_target_len, padding='post')\n",
    "onehotted_decoder_output_arr=tf.one_hot(padded_decoder_output_arr, len(target_vocab)+1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context-vector length\n",
    "latent_dim=ctx_vec_len\n",
    "\n",
    "# this is the source languge consumtion layer\n",
    "encoder_inputs = Input(shape=(None,), name='encoder_sources')\n",
    "# embed the 2-d source into 3-d\n",
    "enc_emb =  Embedding(len(source_vocab)+1, latent_dim, mask_zero = True, name='enc_emb')(encoder_inputs)\n",
    "\n",
    "# LSTM layer to encode the source sentence into context-vector representation\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True, name='encoder_lstm')\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "# encoded-states tensor stores the context-vector\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the target languge consumtion layer\n",
    "decoder_inputs = Input(shape=(None,), name='decoder_sources')\n",
    "# embed the 2-d source into 3-d\n",
    "dec_emb_layer = Embedding(len(target_vocab)+1, latent_dim, mask_zero = True, name='dec_emb_layer')\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# decoder LSTM, this takes in the context-vector and starting or so-far decoded part of the target sentence\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "\n",
    "# final layer that gives a probabilty distribution of the next possible words\n",
    "decoder_dense = Dense(len(target_vocab)+1, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_Translation\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_sources (InputLayer)    [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_sources (InputLayer)    [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_emb (Embedding)             (None, None, 256)    1115392     encoder_sources[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dec_emb_layer (Embedding)       (None, None, 256)    1818624     decoder_sources[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm (LSTM)             [(None, 256), (None, 525312      enc_emb[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 256),  525312      dec_emb_layer[0][0]              \n",
      "                                                                 encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 7104)   1825728     decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 5,810,368\n",
      "Trainable params: 5,810,368\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model building and summary\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs, name='Model_Translation')\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard Callback \n",
    "tbCallBack = TensorBoard(log_dir=os.path.join(root_dir, 'Graph'), histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if training:\n",
    "    # train the model\n",
    "    history=model.fit([padded_encoder_source_arr, padded_decoder_source_arr], onehotted_decoder_output_arr, epochs=epochs, validation_split=0.2, callbacks=[tbCallBack])\n",
    "    model.save_weights(os.path.join(root_dir, 'word-seq2seq.hdf5'))\n",
    "    with plt.style.context('dark_background'):\n",
    "        plt.plot(history.history['acc'])\n",
    "        plt.plot(history.history['val_acc'])\n",
    "        plt.title('model accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "        plt.show()\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the source sequence to get the \"Context vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_state_input = [decoder_state_input_h, decoder_state_input_c]\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_state_input)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_state_input,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(source_seq):\n",
    "    \n",
    "    # Encode the source as state vectors.\n",
    "    states_value = encoder_model.predict(source_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of \n",
    "    #target sequence with the start character.\n",
    "    target_seq[0, 0] = target_wordint_rel.inv['START_']\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "    # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word =target_wordint_rel[sampled_token_index]\n",
    "        decoded_sentence += [sampled_word]\n",
    "    # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_word == '_END' or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "    # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "    # Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg error for 10 tests was 1.0308702786361157.\n",
      "                                          Expected                   Predicted\n",
      "0  il était doté d'une désagréable voix perçante .   je ne suis pas de de de .\n",
      "1            il avait peu d'amis et peu d'argent .   je ne suis pas de de de .\n",
      "2     il avait beaucoup d'argent pour son voyage .  je ne suis pas pas de de .\n",
      "3     il eut l'effronterie d'ignorer mon conseil .     il a a de de de de de .\n",
      "4      il a une profonde affection pour son fils .  je ne suis pas pas de de .\n",
      "5                il a bien plus d'argent que moi .     il a a de de de de de .\n",
      "6            il a beaucoup plus d'argent que moi .  il a a de de de de de de .\n",
      "7        il a une parfaite maîtrise de l'anglais .     il a a de de de de de .\n",
      "8                il a tendance à être pessimiste .  il a a de de de de de de .\n",
      "9                il a été occupé depuis ce matin .   je ne ne pas pas pas de .\n"
     ]
    }
   ],
   "source": [
    "def calc_strdiff(true, pred):\n",
    "    return sum([1 for char in list(difflib.ndiff(true, pred)) if '+ ' in char or '- ' in char])/(len(true))\n",
    "    \n",
    "if not training:\n",
    "    \n",
    "    model.load_weights(os.path.join(root_dir, 'word-seq2seq.hdf5'))\n",
    "    y_truePred = [(' '.join(target_sents[seq_index][1:-1]), ' '.join(decode_sequence(padded_encoder_source_arr[seq_index:seq_index+1])[:-1])) for seq_index, _ in enumerate(padded_encoder_source_arr[:no_of_tests])]\n",
    "    error=sum([calc_strdiff(true, pred) for true, pred in y_truePred])/no_of_tests\n",
    "    print(f'Avg error for {no_of_tests} tests was {error}.')\n",
    "    print(pd.DataFrame(y_truePred, columns=['Expected', 'Predicted']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
